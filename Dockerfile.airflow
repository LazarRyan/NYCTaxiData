FROM apache/airflow:2.8.1

USER root

# Install system dependencies
RUN apt-get update && apt-get install -y \
    default-jdk \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/default-java
ENV PATH=$PATH:$JAVA_HOME/bin

# Download and setup Spark (as root)
RUN wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz \
    && tar -xzf spark-3.5.0-bin-hadoop3.tgz \
    && mv spark-3.5.0-bin-hadoop3 /opt/spark \
    && rm spark-3.5.0-bin-hadoop3.tgz \
    && chown -R airflow:root /opt/spark

# Set Spark environment variables
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin
ENV PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
ENV SPARK_CONF_DIR=$SPARK_HOME/conf

# Switch to airflow user for pip installs
USER airflow

# Install PySpark and dependencies
RUN pip install --no-cache-dir \
    pyspark==3.5.0 \
    pyarrow==14.0.2 \
    psycopg2-binary==2.9.9 \
    requests==2.31.0

# Create spark_jobs directory
RUN mkdir -p /opt/airflow/spark_jobs 